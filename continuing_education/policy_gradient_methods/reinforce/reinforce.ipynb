{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce\n",
    "\n",
    "Tutorial: https://huggingface.co/learn/deep-rl-course/unit4/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "First we will create the cartpole environment.\n",
    "\n",
    "The observation_space of cartpole is a 4-dimensional float vector,\n",
    "and the action_space is a discrete space with 2 possible actions (left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: (4,)\n",
      "Action size: 2\n",
      "Example state: (array([ 0.03978645, -0.01053815, -0.00980605, -0.04763197], dtype=float32), {})\n",
      "Action return: (array([ 0.03957569,  0.18472303, -0.01075869, -0.34339258], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan.peach/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space_shape = env.observation_space.shape\n",
    "action_space_size = env.action_space.n  # type: ignore\n",
    "print(\"State size:\", observation_space_shape)\n",
    "print(\"Action size:\", action_space_size)\n",
    "state = env.reset()\n",
    "print(f\"Example state: {state}\")\n",
    "action_return = env.step(1)\n",
    "print(f\"Action return: {action_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is the policy network, in the paper represented by $\\pi_{\\theta}(s_t)$\n",
    "\n",
    "Meaning the policy $\\pi$ given the parameters $\\theta$ (which in this code\n",
    "represents the weights and biases of self.input, self.hidden and self.output) when\n",
    "doing a forward pass with the state $s$ at time $t$ as input.\n",
    "\n",
    "The network is very simple feed forward network, with relu activation functions and a softmax output.\n",
    "\n",
    "The output of the forward method is what the paper calls $\\pi_{\\theta}(a_i | s_t)$, which is a PDF due to the `softmax`.\n",
    "\n",
    "The action method is a translation from a numpy state vector into an int action, using the forward pass of the network and the REINFORCE score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "\n",
    "# Lets make some types to make type annotation easier\n",
    "State = NewType(\"State\", npt.NDArray[np.float64])\n",
    "Action = NewType(\"Action\", int)\n",
    "Reward = NewType(\"Reward\", float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"A classic policy network is one which takes in a state\n",
    "    and returns a probability distribution over the action space\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, state_size: int, action_size: int, hidden_sizes: List[int]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This is a very simple feed forward network\n",
    "        with an input of size state_size, and output of size action_size\n",
    "        and ReLU activations between the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(hidden_sizes) > 0, \"Need at least one hidden layer\"\n",
    "        network = [nn.Linear(state_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            network.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Linear(hidden_sizes[-1], action_size))\n",
    "        network.append(nn.Softmax())\n",
    "        self.network = nn.Sequential(*network)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes a state tensor and returns a probability distribution along the action space\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "    def act(self, state: State) -> Tuple[Action, float]:\n",
    "        \"\"\"Same as forward, instead of returning the entire distribution, we\n",
    "        return the maximum probability action\n",
    "        along with the log probability of that action\n",
    "        \"\"\"\n",
    "        # First we got to convert out of numpy and into pytorch\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Now we can run the forward pass, whos output is a probability distribution\n",
    "        # along the action space\n",
    "        pdf = self.forward(state_tensor).cpu()\n",
    "\n",
    "        # Now we want to get the action that corresponds to the highest probability\n",
    "        action_idx = np.argmax(pdf)\n",
    "\n",
    "        # We return the action and the log probability of the action\n",
    "        return Action(action_idx.item()), np.log(pdf[action_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - REINFORCE\n",
    "\n",
    "This is the training loop for the REINFORCE algorithm.\n",
    "\n",
    "Training is done by assembling a sample of trajectories, which are lists of tuples of (state, action, reward).\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Start with policy model $\\pi_{\\theta}$\n",
    "2. repeat:\n",
    "    1. Generate an episode $S_0, A_0, r_0, ..., S_{T-1}, A_{T-1}, r_{T-1}$ following $\\pi_{\\theta}$\n",
    "    2. for t from T-1 to 0:\n",
    "        1. $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k$\n",
    "    3. $L(\\theta) = \\frac{1}{T} \\sum_{t=0}^{T-1} G_t \\log \\pi_{\\theta}(a_t | s_t)$\n",
    "    4. Optimize $\\pi_{\\theta}$ using $\\nabla_{\\theta} L(\\theta)$\n",
    "\n",
    "First lets create some helper functions and types to use in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# SAR stands for State, Action, Reward\n",
    "@dataclass\n",
    "class SAR:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: Reward\n",
    "    log_prob: float\n",
    "\n",
    "\n",
    "# A list of SAR representing a single episode\n",
    "Trajectory = NewType(\"Trajectory\", List[SAR])\n",
    "# A list of just the rewards from a single episode\n",
    "RewardTrajectory = NewType(\"RewardTrajectory\", List[Reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(policy: Policy) -> Tuple[Trajectory, Reward]:\n",
    "    \"\"\"2.1. Returns the trajectory and the sum of all rewards.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        action, log_prob = policy.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        trajectory.append(\n",
    "            SAR(\n",
    "                state=State(state),\n",
    "                action=action,\n",
    "                reward=Reward(reward),\n",
    "                log_prob=log_prob,\n",
    "            )\n",
    "        )\n",
    "    return Trajectory(trajectory), Reward(sum(sar.reward for sar in trajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0812a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_discounted_reward(\n",
    "    trajectory: RewardTrajectory, gamma: float = 0.5\n",
    ") -> RewardTrajectory:\n",
    "    \"\"\"2.2.1 Returns the cumulative discounted rewards of a trajectory for each timestep.\"\"\"\n",
    "    if len(trajectory) == 0:\n",
    "        raise ValueError(\"Trajectory needs at least one item.\")\n",
    "    if len(trajectory) == 1:\n",
    "        return RewardTrajectory([trajectory[0]])\n",
    "    discounted_rewards: List[Reward] = []\n",
    "    cumulative_reward: Reward = 0\n",
    "    for reward in reversed(trajectory):\n",
    "        cumulative_reward = reward + gamma * cumulative_reward\n",
    "        discounted_rewards.append(cumulative_reward)\n",
    "    return RewardTrajectory(discounted_rewards[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d906968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "import pytest\n",
    "\n",
    "\n",
    "# Its important to test equations like this!\n",
    "@pytest.mark.parametrize(\n",
    "    \"test_input,expected\",\n",
    "    [([0], [0]), ([1, 1], [1.5, 1]), ([1, 1, 1], [1.75, 1.5, 1])],\n",
    ")\n",
    "def test_cumulative_discounted_reward(\n",
    "    test_input: RewardTrajectory, expected: RewardTrajectory\n",
    ") -> None:\n",
    "    assert cumulative_discounted_reward(test_input, gamma=0.5) == expected"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "continuing-education-vJKa4-To-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
