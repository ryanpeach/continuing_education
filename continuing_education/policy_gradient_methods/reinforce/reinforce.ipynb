{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2cd6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce\n",
    "\n",
    "Tutorial: https://huggingface.co/learn/deep-rl-course/unit4/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "First we will create the cartpole environment.\n",
    "\n",
    "The observation_space of cartpole is a 4-dimensional float vector,\n",
    "and the action_space is a discrete space with 2 possible actions (left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: (4,)\n",
      "Action size: 2\n",
      "Example state: (array([ 0.03551561, -0.01071191,  0.02813781,  0.01822949], dtype=float32), {})\n",
      "Action return: (array([ 0.03530137,  0.18399544,  0.0285024 , -0.26544452], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan.peach/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space_shape = env.observation_space.shape\n",
    "action_space_size = env.action_space.n  # type: ignore\n",
    "print(\"State size:\", observation_space_shape)\n",
    "print(\"Action size:\", action_space_size)\n",
    "state = env.reset()\n",
    "print(f\"Example state: {state}\")\n",
    "action_return = env.step(1)\n",
    "print(f\"Action return: {action_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is the policy network, in the paper represented by $\\pi_{\\theta}(s_t)$\n",
    "\n",
    "Meaning the policy $\\pi$ given the parameters $\\theta$ (which in this code\n",
    "represents the weights and biases of self.input, self.hidden and self.output) when\n",
    "doing a forward pass with the state $s$ at time $t$ as input.\n",
    "\n",
    "The network is very simple feed forward network, with relu activation functions and a softmax output.\n",
    "\n",
    "The output of the forward method is what the paper calls $\\pi_{\\theta}(a_i | s_t)$, which is a PDF due to the `softmax`.\n",
    "\n",
    "The action method is a translation from a numpy state vector into an int action, using the forward pass of the network and the REINFORCE score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "# Lets make some types to make type annotation easier\n",
    "State = NewType(\"State\", npt.NDArray[np.float64])\n",
    "Action = NewType(\"Action\", int)\n",
    "Reward = NewType(\"Reward\", float)\n",
    "LogProb = NewType(\"LogProb\", Tensor)\n",
    "LogLikelihood = NewType(\"LogLikelihood\", Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"A classic policy network is one which takes in a state\n",
    "    and returns a probability distribution over the action space\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, state_size: int, action_size: int, hidden_sizes: List[int]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This is a very simple feed forward network\n",
    "        with an input of size state_size, and output of size action_size\n",
    "        and ReLU activations between the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(hidden_sizes) > 0, \"Need at least one hidden layer\"\n",
    "        network = [nn.Linear(state_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            network.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Linear(hidden_sizes[-1], action_size))\n",
    "        network.append(nn.Softmax())\n",
    "        self.network = nn.Sequential(*network)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes a state tensor and returns a probability distribution along the action space\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def act(self, state: State) -> Tuple[Action, LogProb]:\n",
    "        \"\"\"Same as forward, instead of returning the entire distribution, we\n",
    "        return the maximum probability action\n",
    "        along with the log probability of that action\n",
    "        \"\"\"\n",
    "        # First we got to convert out of numpy and into pytorch\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Now we can run the forward pass, whos output is a probability distribution\n",
    "        # along the action space\n",
    "        pdf = self.forward(state_tensor).cpu()\n",
    "\n",
    "        # Now we want to get the action that corresponds to the highest probability\n",
    "        action_idx = np.argmax(pdf)\n",
    "\n",
    "        # However, we are going to do backprop through the log probability of the action\n",
    "        # Therefore this needs to stay as a tensor\n",
    "        # The Category distribution in torch has a method for a backprop friendly log probability of one action from a multinomial distribution\n",
    "        log_prob = torch.distributions.Categorical(pdf).log_prob(action_idx)\n",
    "\n",
    "        # We return the action and the log probability of the action\n",
    "        return Action(action_idx.item()), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - REINFORCE\n",
    "\n",
    "This is the training loop for the REINFORCE algorithm.\n",
    "\n",
    "Training is done by assembling a sample of trajectories, which are lists of tuples of (state, action, reward).\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Start with policy model $\\pi_{\\theta}$\n",
    "2. repeat:\n",
    "    1. Generate an episode $S_0, A_0, r_0, ..., S_{T-1}, A_{T-1}, r_{T-1}$ following $\\pi_{\\theta}$\n",
    "    2. for t from T-1 to 0:\n",
    "        1. $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k$\n",
    "    3. $L(\\theta) = \\frac{1}{T} \\sum_{t=0}^{T-1} G_t \\log \\pi_{\\theta}(a_t | s_t)$\n",
    "    4. Optimize $\\pi_{\\theta}$ using $\\nabla_{\\theta} L(\\theta)$\n",
    "\n",
    "First lets create some helper functions and types to use in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# SAR stands for State, Action, Reward\n",
    "@dataclass\n",
    "class SAR:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: Reward\n",
    "    log_prob: LogProb\n",
    "\n",
    "\n",
    "# A list of SAR representing a single episode\n",
    "Trajectory = NewType(\"Trajectory\", List[SAR])\n",
    "# A list of just the rewards from a single episode\n",
    "RewardTrajectory = NewType(\"RewardTrajectory\", List[Reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(policy: Policy) -> Trajectory:\n",
    "    \"\"\"2.1. Returns the trajectory and the sum of all rewards.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        action, log_prob = policy.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        trajectory.append(\n",
    "            SAR(\n",
    "                state=State(state),\n",
    "                action=action,\n",
    "                reward=Reward(reward),\n",
    "                log_prob=LogProb(log_prob),\n",
    "            )\n",
    "        )\n",
    "    return Trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce0812a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_discounted_rewards(\n",
    "    trajectory: RewardTrajectory, gamma: float = 0.5\n",
    ") -> RewardTrajectory:\n",
    "    \"\"\"2.2.1 Returns the cumulative discounted rewards of a trajectory for each timestep.\"\"\"\n",
    "    if len(trajectory) == 0:\n",
    "        raise ValueError(\"Trajectory needs at least one item.\")\n",
    "    if len(trajectory) == 1:\n",
    "        return RewardTrajectory([trajectory[0]])\n",
    "    discounted_rewards: List[Reward] = []\n",
    "    cumulative_reward: Reward = Reward(0)\n",
    "    for reward in reversed(trajectory):\n",
    "        cumulative_reward = Reward(reward + gamma * cumulative_reward)\n",
    "        discounted_rewards.append(cumulative_reward)\n",
    "    return RewardTrajectory(discounted_rewards[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d906968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to test our code, so we know it works as expected\n",
    "# We tried to use ipytest but it wasn't working https://github.com/chmp/ipytest\n",
    "assert cumulative_discounted_rewards(RewardTrajectory([0]), gamma=0.5) == RewardTrajectory([0])\n",
    "assert cumulative_discounted_rewards(RewardTrajectory([1]), gamma=0.5) == RewardTrajectory([1])\n",
    "assert cumulative_discounted_rewards(RewardTrajectory([1, 1]), gamma=0.5) == RewardTrajectory([1.5, 1])\n",
    "assert cumulative_discounted_rewards(RewardTrajectory([1, 1, 1]), gamma=0.5) == RewardTrajectory([1.75, 1.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(returns: Tensor) -> Tensor:\n",
    "    ## standardization of the returns is employed to make training more stable\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    ## eps is the smallest representable float, which is\n",
    "    # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(policy: Policy, trajectory: Trajectory) -> LogLikelihood:\n",
    "    \"\"\"\n",
    "    2.2.2 Returns the likelihood of a trajectory given a policy.\n",
    "    Instead of doing 1/T, we normalize the cumulative discounted rewards as it says\n",
    "    to do in the tutorial.\n",
    "    Also we use torch.cat and sum for backprop reasons\n",
    "    \"\"\"\n",
    "    log_likelihoods = []\n",
    "    cum_disc_rewards = normalize(cumulative_discounted_rewards(RewardTrajectory([sar.reward for sar in trajectory])))\n",
    "    for cum_disc_reward, sar in zip(cum_disc_rewards, trajectory):\n",
    "        _, log_prob = policy.act(sar.state)\n",
    "        log_likelihoods.append(cum_disc_reward * -log_prob)\n",
    "    return LogLikelihood(torch.cat(log_likelihoods).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cartpole_hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_training_episodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_space_size,\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mPolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcartpole_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcartpole_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcartpole_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcartpole_hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mPolicy.__init__\u001b[0;34m(self, state_size, action_size, hidden_sizes)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hidden_sizes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least one hidden layer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m network \u001b[38;5;241m=\u001b[39m [\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, nn\u001b[38;5;241m.\u001b[39mReLU()]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hidden_sizes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m     network\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mLinear(hidden_sizes[i], hidden_sizes[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\""
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"state_space\": observation_space_shape,\n",
    "    \"action_space\": action_space_size,\n",
    "}\n",
    "policy = Policy(\n",
    "    cartpole_hyperparameters[\"state_space\"],\n",
    "    cartpole_hyperparameters[\"action_space\"],\n",
    "    [cartpole_hyperparameters[\"h_size\"]],\n",
    ").to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     trajectory \u001b[38;5;241m=\u001b[39m collect_episode(policy)\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mPolicy.__init__\u001b[0;34m(self, state_size, action_size, hidden_sizes)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hidden_sizes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least one hidden layer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m network \u001b[38;5;241m=\u001b[39m [\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, nn\u001b[38;5;241m.\u001b[39mReLU()]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hidden_sizes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m     network\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mLinear(hidden_sizes[i], hidden_sizes[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got tuple\""
     ]
    }
   ],
   "source": [
    "for i in range(cartpole_hyperparameters[\"n_training_episodes\"]):\n",
    "    trajectory = collect_episode(policy)\n",
    "    policy_loss = log_likelihood(policy, trajectory)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "continuing-education-vJKa4-To-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
