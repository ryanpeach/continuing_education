{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce\n",
    "\n",
    "Tutorial: https://huggingface.co/learn/deep-rl-course/unit4/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "First we will create the cartpole environment.\n",
    "\n",
    "The observation_space of cartpole is a 4-dimensional float vector,\n",
    "and the action_space is a discrete space with 2 possible actions (left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: (4,)\n",
      "Action size: 2\n",
      "Example state: (array([-8.2978457e-03, -1.8829281e-02, -2.8209276e-02, -3.4900859e-05],\n",
      "      dtype=float32), {})\n",
      "Action return: (array([-0.00867443,  0.17668563, -0.02820997, -0.30148304], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan.peach/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning:\n",
      "\n",
      "`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space_shape = env.observation_space.shape\n",
    "action_space_size = env.action_space.n  # type: ignore\n",
    "print(\"State size:\", observation_space_shape)\n",
    "print(\"Action size:\", action_space_size)\n",
    "state = env.reset()\n",
    "print(f\"Example state: {state}\")\n",
    "action_return = env.step(1)\n",
    "print(f\"Action return: {action_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is the policy network, in the paper represented by $\\pi_{\\theta}(s_t)$\n",
    "\n",
    "Meaning the policy $\\pi$ given the parameters $\\theta$ (which in this code\n",
    "represents the weights and biases of self.input, self.hidden and self.output) when\n",
    "doing a forward pass with the state $s$ at time $t$ as input.\n",
    "\n",
    "The network is very simple feed forward network, with relu activation functions and a softmax output.\n",
    "\n",
    "The output of the forward method is what the paper calls $\\pi_{\\theta}(a_i | s_t)$, which is a PDF due to the `softmax`.\n",
    "\n",
    "The action method is a translation from a numpy state vector into an int action, using the forward pass of the network and the REINFORCE score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "\n",
    "# Lets make some types to make type annotation easier\n",
    "State = NewType(\"State\", npt.NDArray[np.float64])\n",
    "Action = NewType(\"Action\", int)\n",
    "Reward = NewType(\"Reward\", float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"A classic policy network is one which takes in a state\n",
    "    and returns a probability distribution over the action space\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, state_size: int, action_size: int, hidden_sizes: List[int]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This is a very simple feed forward network\n",
    "        with an input of size state_size, and output of size action_size\n",
    "        and ReLU activations between the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(hidden_sizes) > 0, \"Need at least one hidden layer\"\n",
    "        network = [nn.Linear(state_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            network.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Linear(hidden_sizes[-1], action_size))\n",
    "        network.append(nn.Softmax())\n",
    "        self.network = nn.Sequential(*network)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes a state tensor and returns a probability distribution along the action space\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "    def act(self, state: State) -> Tuple[Action, float]:\n",
    "        \"\"\"Same as forward, instead of returning the entire distribution, we\n",
    "        return the maximum probability action\n",
    "        along with the log probability of that action\n",
    "        \"\"\"\n",
    "        # First we got to convert out of numpy and into pytorch\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Now we can run the forward pass, whos output is a probability distribution\n",
    "        # along the action space\n",
    "        pdf = self.forward(state_tensor).cpu()\n",
    "\n",
    "        # Now we want to get the action that corresponds to the highest probability\n",
    "        action_idx = np.argmax(pdf)\n",
    "\n",
    "        # We return the action and the log probability of the action\n",
    "        return Action(action_idx.item()), np.log(pdf[action_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - REINFORCE\n",
    "\n",
    "This is the training loop for the REINFORCE algorithm.\n",
    "\n",
    "Training is done by assembling a sample of trajectories, which are lists of tuples of (state, action, reward).\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Start with policy model $\\pi_{\\theta}$\n",
    "2. repeat:\n",
    "    1. Generate an episode $S_0, A_0, r_1, ..., S_{T-1}, A_{T-1}, r_{T-1}$ following $\\pi_{\\theta}$\n",
    "    2. for t from T-1 to 0:\n",
    "        1. $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_k$\n",
    "    3. $L(\\theta) = \\frac{1}{T} \\sum_{t=0}^{T-1} G_t \\log \\pi_{\\theta}(a_t | s_t)$\n",
    "    4. Optimize $\\pi_{\\theta}$ using $\\nabla_{\\theta} L(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# SAR stands for State, Action, Reward\n",
    "@dataclass\n",
    "class SAR:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: Reward\n",
    "    log_prob: float\n",
    "\n",
    "\n",
    "# A list of SAR representing a single episode\n",
    "Trajectory = NewType(\"Trajectory\", List[SAR])\n",
    "# A list of just the rewards from a single episode\n",
    "RewardTrajectory = NewType(\"RewardTrajectory\", List[Reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def collect_episode(policy: Policy) -> Tuple[Trajectory, Reward]:\n",
    "    \"\"\"Returns the trajectory and the sum of all rewards.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        action, log_prob = policy.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        trajectory.append(\n",
    "            SAR(\n",
    "                state=State(state),\n",
    "                action=action,\n",
    "                reward=Reward(reward),\n",
    "                log_prob=log_prob,\n",
    "            )\n",
    "        )\n",
    "    return Trajectory(trajectory), Reward(sum(sar.reward for sar in trajectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This represents the formula $R(\\tau)$ in the tutorial. It's a simple reward decay formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "def cumulative_discounted_reward(\n",
    "    trajectory: RewardTrajectory, gamma: float = 0.5\n",
    ") -> Reward:\n",
    "    if len(trajectory) == 0:\n",
    "        raise ValueError(\"Trajectory needs at least one item.\")\n",
    "    if len(trajectory) == 1:\n",
    "        return 0.0\n",
    "    out = trajectory[1]\n",
    "    if len(trajectory) == 2:\n",
    "        return out\n",
    "    for i in range(2, len(trajectory)):\n",
    "        out += gamma * trajectory[i]\n",
    "        gamma *= gamma\n",
    "    return out\n",
    "\n",
    "\n",
    "# Its important to test equations like this!\n",
    "@pytest.mark.parametrize(\n",
    "    \"test_input,expected\",\n",
    "    [([0], 0), ([1, 1], 1), ([1, 1, 1], 1.5), ([1, 1, 1, 1], 1.75)],\n",
    ")\n",
    "def test_cumulative_discounted_reward(\n",
    "    test_input: RewardTrajectory, expected: float\n",
    ") -> None:\n",
    "    assert cumulative_discounted_reward(test_input, gamma=0.5) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.1.1, pluggy-1.4.0 -- /Users/ryan.peach/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/ryan.peach/Documents/ryanpeach/continuing_education\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-4.3.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 8 items\n",
      "\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_return[test_input0-0] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 12%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_return[test_input1-1] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 25%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_return[test_input2-1.5] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 37%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_return[test_input3-1.75] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 50%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_discounted_reward[test_input0-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_discounted_reward[test_input1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_discounted_reward[test_input2-1.5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "t_e4ca5c3e2c49403986246215c689d707.py::test_cumulative_discounted_reward[test_input3-1.75] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "ipytest.run(\"-vv\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "continuing-education-vJKa4-To-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
