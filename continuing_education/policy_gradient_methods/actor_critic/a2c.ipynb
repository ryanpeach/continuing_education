{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    __this_file = (\n",
    "        Path().resolve() / \"actor_critic.ipynb\"\n",
    "    )  # jupyter does not have __file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Actor Critic\n",
    "\n",
    "A concept that Actor Critic introduces is that it hybridizes policy based methods (the actor) and value based methods (the critic). This gives you many of the advantages of both methods, for example you can use off-policy learning of value based methods (though A2C does not), and the continuous action space and stochasticity of policy based methods is maintained.\n",
    "\n",
    "The actor is a policy that outputs a probability distribution over actions. The critic is a value function that estimates the expected return of a state. The actor is trained to maximize the expected return, while the critic is trained to minimize the error between the estimated return and the actual return.\n",
    "\n",
    "The objective function is the same as in REINFORCE, except we use the value network to estimate the average return, rather than a batch average, and we subtract the value from the return to get the advantage. This stabilizes the scaling of the policy gradient by having a better baseline.\n",
    "\n",
    "The huggingface tutorial on A2C wants us to use stable-baselines3, but I call that cheating. We will implement A2C from scratch using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuing_education.policy_gradient_methods.reinforce import SamplePolicy\n",
    "\n",
    "\n",
    "class Actor(SamplePolicy):\n",
    "    \"\"\"This is exactly the same as we use in REINFORCE.\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from continuing_education.lib.episodes import SARS\n",
    "\n",
    "\n",
    "class ValueCritic:\n",
    "    \"\"\"This is a value network, rather than a Q network, to reduce the number of samples needed to train the network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, state_size: int, hidden_sizes: list[int], gamma: float\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        assert len(hidden_sizes) > 0, \"Need at least one hidden layer\"\n",
    "        self.state_size = state_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        # Dimensions in the network are (batch_size, input_size, 1)\n",
    "        network: list[nn.Module] = []\n",
    "        network.append(\n",
    "            nn.Linear(state_size, hidden_sizes[0])\n",
    "        )  # Shape: (:, state_size, hidden_sizes[0])\n",
    "        network.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            network.append(\n",
    "                nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])\n",
    "            )  # Shape: (:, hidden_sizes[i], hidden_sizes[i+1])\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(\n",
    "            nn.Linear(hidden_sizes[-1], 1)\n",
    "        )  # Shape: (:, hidden_sizes[-1], 1)\n",
    "        self.network = nn.Sequential(*network).to(DEVICE)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes a state tensor and returns logits along the action space\"\"\"\n",
    "        state = state.to(DEVICE)\n",
    "        return self.network(state)\n",
    "\n",
    "    def advantage(self, sars: SARS) -> torch.Tensor:\n",
    "        \"\"\"Computes the advantage of the current state, next state, and reward.\"\"\"\n",
    "        state = torch.from_numpy(sars.state).to(DEVICE)\n",
    "        next_state = torch.from_numpy(sars.next_state).to(DEVICE)\n",
    "        return sars.reward + self.gamma * self.forward(next_state) - self.forward(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., â€¦ Kavukcuoglu, K. (2016). Asynchronous Methods for Deep Reinforcement Learning. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1602.01783\n",
    "2. https://huggingface.co/blog/deep-rl-a2c\n",
    "3. UNIT 6. ACTOR CRITIC METHODS WITH ROBOTICS ENVIRONMENTS. Hugging Face. (n.d.). https://huggingface.co/learn/deep-rl-course/unit6/introduction\n",
    "4. https://samuelebolotta.medium.com/3-actor-critic-algorithms-779f14465b74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "continuing_education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
