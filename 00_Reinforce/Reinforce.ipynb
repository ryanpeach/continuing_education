{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce\n",
    "\n",
    "Tutorial: https://huggingface.co/learn/deep-rl-course/unit4/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "First we will create the cartpole environment.\n",
    "\n",
    "The observation_space of cartpole is a 4-dimensional float vector,\n",
    "and the action_space is a discrete space with 2 possible actions (left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: (4,)\n",
      "Action size: 2\n",
      "Example state: (array([-0.0437701 ,  0.0017786 , -0.00340019, -0.02410924], dtype=float32), {})\n",
      "Action return: (array([-0.04373452,  0.19694914, -0.00388237, -0.31786302], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan.peach/Library/Caches/pypoetry/virtualenvs/continuing-education-vJKa4-To-py3.10/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_space_shape = env.observation_space.shape\n",
    "action_space_size = env.action_space.n  # type: ignore\n",
    "print(\"State size:\", observation_space_shape)\n",
    "print(\"Action size:\", action_space_size)\n",
    "state = env.reset()\n",
    "print(f\"Example state: {state}\")\n",
    "action_return = env.step(1)\n",
    "print(f\"Action return: {action_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is the policy network, in the paper represented by $\\pi_{\\theta}(s_t)$\n",
    "\n",
    "Meaning the policy $\\pi$ given the parameters $\\theta$ (which in this code\n",
    "represents the weights and biases of self.input, self.hidden and self.output) when\n",
    "doing a forward pass with the state $s$ at time $t$ as input.\n",
    "\n",
    "The network is very simple feed forward network, with relu activation functions and a softmax output.\n",
    "\n",
    "The output of the forward method is what the paper calls $\\pi_{\\theta}(a_i | s_t)$, which is a PDF due to the `softmax`.\n",
    "\n",
    "The action method is a translation from a numpy state vector into an int action, using the forward pass of the network and the REINFORCE score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "\n",
    "# Lets make some types to make type annotation easier\n",
    "State = NewType(\"State\", npt.NDArray[np.float64])\n",
    "Action = NewType(\"Action\", int)\n",
    "Reward = NewType(\"Reward\", float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self, state_size: int, action_size: int, hidden_sizes: List[int]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert len(hidden_sizes) > 0, \"Need at least one hidden layer\"\n",
    "        network = [nn.Linear(state_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            network.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Linear(hidden_sizes[-1], action_size))\n",
    "        network.append(nn.Softmax())\n",
    "        self.network = nn.Sequential(*network)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "    def act(self, state: State) -> Tuple[Action, float]:\n",
    "        # First we got to convert out of numpy and into pytorch\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        print(f\"state: {state}\")\n",
    "        print(f\"state_unsqueezed: {np.expand_dims(state, axis=0)}\")\n",
    "        pdf = self.forward(\n",
    "            state_tensor\n",
    "        ).cpu()  # TODO: If softmax produces a PDF, why do we need Categorical to sample from it?\n",
    "        print(f\"PDF: {pdf}\")\n",
    "        multinomial = Categorical(\n",
    "            pdf\n",
    "        )  # TODO: Study up on multinomial distributions and log probs\n",
    "        # TODO: Take the argmax of the pdf instead and see what happens!\n",
    "        print(f\"Multinomial: {multinomial}\")\n",
    "        action_idx = np.argmax(multinomial)  # type: ignore\n",
    "        return Action(action_idx.item()), multinomial.log_prob(action_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training is done by assembling a sample of trajectories, which are lists of tuples of (state, action, reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SAR:\n",
    "    state: State\n",
    "    action: Action\n",
    "    reward: Reward\n",
    "    log_prob: float\n",
    "\n",
    "\n",
    "Trajectory = NewType(\"Trajectory\", List[SAR])\n",
    "RewardTrajectory = NewType(\"RewardTrajectory\", List[Reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(policy: Policy) -> Tuple[Trajectory, Reward]:\n",
    "    \"\"\"Returns the trajectory and the sum of all rewards.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        action, log_prob = policy.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        trajectory.append(\n",
    "            SAR(\n",
    "                state=State(state),\n",
    "                action=action,\n",
    "                reward=Reward(reward),\n",
    "                log_prob=log_prob,\n",
    "            )\n",
    "        )\n",
    "    return Trajectory(trajectory), Reward(sum(sar.reward for sar in trajectory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the formula $R(\\tau)$ in the tutorial. It's a simple reward decay formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "def cumulative_return(trajectory: RewardTrajectory, gamma: float = 0.5) -> float:\n",
    "    if len(trajectory) == 0:\n",
    "        raise ValueError(\"Trajectory needs at least one item.\")\n",
    "    if len(trajectory) == 1:\n",
    "        return 0.0\n",
    "    out: float = trajectory[1]\n",
    "    if len(trajectory) == 2:\n",
    "        return out\n",
    "    for i in range(2, len(trajectory)):\n",
    "        out += gamma * trajectory[i]\n",
    "        gamma *= gamma\n",
    "    return out\n",
    "\n",
    "\n",
    "# Its important to test equations like this!\n",
    "@pytest.mark.parametrize(\n",
    "    \"test_input,expected\",\n",
    "    [([0], 0), ([1, 1], 1), ([1, 1, 1], 1.5), ([1, 1, 1, 1], 1.75)],\n",
    ")\n",
    "def test_cumulative_return(test_input: RewardTrajectory, expected: float) -> None:\n",
    "    assert cumulative_return(test_input, gamma=0.5) == expected"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "continuing-education-vJKa4-To-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
